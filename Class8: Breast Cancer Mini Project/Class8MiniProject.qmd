---
title: "Class 8: Breast Cancer Mini-Project"
author: "Bernice Lozada (A16297973)"
format: pdf
---
## About
Today's lab uses unsupervised learning to analyze data from the Wisconsin Breast Cancer Diagnostic Data Set, obtained by the University of Wisconsin Medical Center. The data set presents characteristics of the cell nuclei from images obtained via fine needle biopsy of breast masses.

## Data Import

```{r}
# Save your input data file into your Project directory
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)

# Store diagnosis as separate vector
diagnosis <- as.factor(wisc.df$diagnosis)

# Get rid of first column
wisc.data <- wisc.df[,-1]
head(wisc.df)
```

## Initial Analysis

> Q1. How many patients/individuals/samples are in this dataset?

569 patients

```{r}
nrow(wisc.df)
```

> Q2. How many of the observations have a malignant diagnosis?

212 observations
```{r}
sum(wisc.df$diagnosis == "M")
```

> Q3. How many variables/features in the data are suffixed with _mean?

10 

```{r}
length(grep("_mean", colnames(wisc.df)))
```


## Clustering

We can try a `kmeans()` first.

```{r}
km <- kmeans(wisc.data, centers = 2)
table(km$cluster, diagnosis)

```
Let's try `hclust()`. The key input for `hclust()` is a distance matrix produced by the `dist()` function. 

```{r}
#use scale to account for different units - but that's why it looks sad :(
hc <- hclust(dist(wisc.data))
plot(hc)
```


## PCA

We can look at the sd of each column (original variables) to determine if we need to scale the data.
```{r}
apply(wisc.data, 2, sd)

```

Since the sd for each column has a large range (area_worst), the largest component will dominate. This means we have to scale the data. We will run `prcomp()` with `scale = TRUE`.

```{r}
wisc.pr <- prcomp(wisc.data, scale = TRUE)
summary(wisc.pr)

```
> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44% of the original variance is captured by PC1. 

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs are required to describe at leaset 70% of the original variance. 

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs are required to describe at least 90% of the original variance. 

Generate our main PCA plot (score plot, PC1 vs PC2 plot)...
```{r}
library(ggplot2)
res <- as.data.frame(wisc.pr$x)

ggplot(res) + aes(x= PC1, y = PC2, col=diagnosis) + geom_point()
```

## Combining Methods

Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with the linkage method="ward.D2". We use Wardâ€™s criterion here because it is based on multidimensional variance like principal components analysis. Assign the results to `wisc.pr.hclust.`

As determined before, this is 7 PCs.

Clustering on PCA Results
```{r}
d <- dist(wisc.pr$x[,1:3])
hc <- hclust(d, method = "ward.D2")
plot(hc)
```

Looking at this plot, cutting the plot around 80 would give two distinct groups.To get my clustering result/membership vector, I need to "cut" the tree with the `cutree()` function.

```{r}
# h = 80 means cut at height 80, k = 2 is cut into two groups
cutree(hc,k = 2)
```
> Q. How many patients in each group?

```{r}

grps <- (cutree(hc,k = 2))
table(grps)
```

```{r}
plot(res$PC1, res$PC2, col = grps)
```

## Prediction

We can use our PCA result (model) to do predictions, or taking new unseen data and project it onto our new PC variables. 

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(res$PC1, res$PC2, col=grps)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> Q18. Which of these new patients should we prioritize for follow up based on your results?

I would prioritize Patient 2 for followup based on my results.

# Summary

Principal Component Analysis (PCA) is a super sueful method for analyzing large datasets. It works by finding new variables (PCs) that capture the most variance from the original variables in your dataset. 